{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "78a1697e883b5fa55847d808bb1b81643f919a46"
   },
   "outputs": [],
   "source": [
    "# script made on 16gb of ram\n",
    "import os\n",
    "import io\n",
    "import os.path\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf      #the progress bar\n",
    "import en_core_web_sm as en  #from the spaCy library, https://spacy.io/usage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "32527401fa956da5514985117d3ba73f78a80b24"
   },
   "outputs": [],
   "source": [
    "filename = \"./data/train.csv\"\n",
    "output = \"./data/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d761b00e58dc29f20f27dd5e9d2faccbd386c678"
   },
   "outputs": [],
   "source": [
    "# enb_path = 'input/embeddings/wiki-news-300d-1M/' + 'wiki-news-300d-1M.vec'\n",
    "# enb_path = 'input/embeddings/GoogleNews-vectors-negative300/'\n",
    "# enb_path = 'input/embeddings/paragram_300_sl999/'\n",
    "enb_path = \"./data/embeddings/glove.840B.300d/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "e9a828213eedcd8ddda4d47e78c9a72174235357"
   },
   "outputs": [],
   "source": [
    "#global variables to save memory\n",
    "# _t is for tokenized\n",
    "embeddings = []      #np array of embeddings\n",
    "vocab = []          #the vocabulary from the enbeding file\n",
    "questions_t = []    #every question tokenized in an array\n",
    "text_t = []         #all the questions concatenated and tokenized\n",
    "word_to_index = {}\n",
    "index_to_word = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e465d9c60ed01f70c9322165f4509282c4f8957b"
   },
   "outputs": [],
   "source": [
    "def tokenize_questions(text,nr_to_delete = 0):\n",
    "    nlp = en.load()      #load the tokenizer\n",
    "    \n",
    "    global questions_t\n",
    "    global vocab         #every unique word in the appearance order  \n",
    "    global text_t       #every token in the original order\n",
    "    \n",
    "    text_t = [] \n",
    "    vocab = [] \n",
    "    questions_t = []\n",
    "    \n",
    "    del text  #delete the original text so we save memory\n",
    "    bar  = tqdm(total = len(batches))\n",
    "    \n",
    "    for batch in batches:  #tqdm is the progress bar\n",
    "        tokens = nlp(batch)\n",
    "        batch_tokenized = []\n",
    "        for token in tokens:\n",
    "            word = token.string.strip()\n",
    "            text_t.append(word)\n",
    "            batch_tokenized.append(word)\n",
    "        questions_t.append(batch_tokenized)\n",
    "        bar.update(1)\n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "30a4e6ffecc691f767afb05e430fe31a9689bc50",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(filename)    #test_t is for now a dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.values[:,2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7db80bc8f30e251d4a0459c55e2400781b8240c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Nr of questions:  1306122\n",
      "Nr of tokens:     19020768\n",
      "The fist question tokenized:  ['How', 'did', 'Quebec', 'nationalists', 'see', 'their', 'province', 'as', 'a', 'nation']\n"
     ]
    }
   ],
   "source": [
    "# loadig the qustions tokenized, and all qustions concatenated tokenized\n",
    "if not os.path.isfile(output + \"questions_t\"):\n",
    "    tokenize_questions( train_df.values[:,1].tolist() )\n",
    "    pickle.dump(questions_t, open(output + \"questions_t\",'wb') )\n",
    "    pickle.dump(text_t, open(output + \"text_t\",'wb') )\n",
    "else:\n",
    "    print(\"Loading data...\")\n",
    "    questions_t = pickle.load( open(output + \"questions_t\", \"rb\" ) )\n",
    "    text_t = pickle.load( open(output + \"text_t\", \"rb\" ) )\n",
    "    print(\"Nr of questions: \",len(questions_t))\n",
    "    print(\"Nr of tokens:    \",len(text_t))\n",
    "    \n",
    "print('The fist question tokenized: ', questions_t[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "42977bc4a0039f3ccb6c1614544faf2e22b01ece"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(enb_path):\n",
    "    global embeddings\n",
    "    global vocab\n",
    "    \n",
    "    embeddings = []\n",
    "    vocab = []\n",
    "    \n",
    "    print(\"Loading embeddings...\")\n",
    "    \n",
    "    with open(enb_path, 'r', encoding ='utf-8') as file:\n",
    "    \n",
    "        num_lines = sum(1 for line in file)\n",
    "        bar  = tqdm(total = num_lines)\n",
    "    \n",
    "        file.seek(0)\n",
    "    \n",
    "        vocab = []\n",
    "        embeddings = []      \n",
    "        for line in file:\n",
    "            bar.update(1)\n",
    "            array = line[:-1].split(' ')\n",
    "            vocab.append(str(array[0]))\n",
    "            embeddings.append(np.array(array[1:]).astype('float32'))      \n",
    "        bar.close()\n",
    "        \n",
    "    embeddings = np.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Embedings size:   (2196017, 300)\n",
      "Vocabulary size:  2196017\n",
      "\n",
      "The word \"the\" with his enmeding: \n",
      " [ 0.27204  -0.06203  -0.1884    0.023225 -0.018158] ...  [-0.018168  0.11407   0.13015  -0.18317   0.1323  ]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(output + \"embeddings\"):\n",
    "    load_embeddings(enb_path)\n",
    "    pickle.dump(embeddings,open( output + \"embeddings\",'wb'))\n",
    "    pickle.dump(vocab,open( output + \"vocabulary\",'wb'))\n",
    "else:\n",
    "    print(\"Loading embeddings...\")\n",
    "    embeddings = pickle.load( open(output + \"embeddings\", \"rb\" ) )\n",
    "    vocab = pickle.load( open( output + \"vocabulary\", \"rb\" ) )\n",
    "print('Embedings size:  ',embeddings.shape)\n",
    "print('Vocabulary size: ', len(vocab))\n",
    "\n",
    "print('\\nThe word \"'+ vocab[2]+'\" with his enmeding: \\n' , embeddings[2][:5], \"... \", embeddings[2][-5:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c36543928269e29af27f41e16d6c848c829e6901"
   },
   "outputs": [],
   "source": [
    "def tokens_to_index():\n",
    "    global word_to_index\n",
    "    global index_to_word\n",
    "    global questions_t\n",
    "    global vocab\n",
    "    \n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    \n",
    "    for idx, word in enumerate(vocab): #create the two dictionaries\n",
    "        word_to_index[word] = idx\n",
    "        index_to_word[idx] = word\n",
    "        \n",
    "    for batch in questions_t: #chage the input from strings to ids\n",
    "        if isinstance(batch, int):\n",
    "            print(batch)\n",
    "        for idx,token in enumerate(batch): #replace the text with the word indexes\n",
    "             batch[idx] = word_to_index.get(token, 132037) #if the token is not in vocab put \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_padding():\n",
    "    global vocab         #we need to add the padding word to our vocabulary\n",
    "    global embeddings    #we need to add the padding word to our embedding matrix\n",
    "    global word_to_index\n",
    "    global index_to_word\n",
    "    \n",
    "    print(\"Adding the padding char to vocab and embeddings...\")\n",
    "    vocab = ['/pad']+vocab\n",
    "    pad_emb = []\n",
    "    for i in range(len(embeddings[0])):\n",
    "        pad_emb.append(0)\n",
    "    pad_emb = np.array(pad_emb)\n",
    "    embeddings = np.vstack([pad_emb,embeddings])\n",
    "    \n",
    "    tokens_to_index()\n",
    "    \n",
    "\n",
    "def pad_questions(data, max_len = 60):\n",
    "    if not (vocab[0] == \"/pad\" and index_to_word[0] == \"/pad\"):\n",
    "        initialze_padding()\n",
    "    \n",
    "    \n",
    "    print(\"Padding questions...\")\n",
    "    data_aux = deepcopy(data)\n",
    "    \n",
    "    padded_count = 0\n",
    "    cut_count = 0\n",
    "    for idx in tqdm(range(len(data_aux))):\n",
    "        question_len = len(data_aux[idx])\n",
    "        if question_len <= max_len:\n",
    "            padded_count += 1\n",
    "            for i in range(question_len, max_len):\n",
    "                data_aux[idx].append(0)\n",
    "        elif question_len > max_len:\n",
    "            cut_count += 1\n",
    "            data_aux[idx] = data_aux[idx][:max_len]\n",
    "        \n",
    "        if len(data_aux[idx])>max_len:\n",
    "            print(question_len)\n",
    "            print(question)\n",
    "            break\n",
    "            \n",
    "    print(\"Padded :\", padded_count)\n",
    "    print(\"Cut    :\", cut_count)\n",
    "    return data_aux\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, data, labels, size = 64):\n",
    "        self.data = data\n",
    "        del data\n",
    "        self.labels = labels\n",
    "        self.size = size\n",
    "        self.index = 0\n",
    "    \n",
    "    def __next__(self):\n",
    "        \n",
    "        if (len(self.data) <= self.index):\n",
    "            raise StopIteration()\n",
    "        \n",
    "        self.index = self.index + self.size    \n",
    "        \n",
    "        return self.data[self.index-self.size:self.index],self.labels[self.index-self.size:self.index]        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return ceil(len(self.data)/self.size)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the padding char to vocab and embeddings...\n",
      "Padding questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:04<00:00, 265729.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded : 1305887\n",
      "Cut    : 235\n"
     ]
    }
   ],
   "source": [
    "questions_t = pad_questions(questions_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(batch):\n",
    "    global embeddings\n",
    "    \n",
    "    batch_embedding = []\n",
    "    \n",
    "    for int_question in batch:\n",
    "        question_embedding = []\n",
    "\n",
    "        for int_word in int_question:\n",
    "            try:\n",
    "                question_embedding.append(embeddings[int_word])\n",
    "            except Exception:\n",
    "                print(int_word)\n",
    "        \n",
    "        enb = np.stack(question_embedding)\n",
    "        batch_embedding.append(enb)\n",
    "    for el in batch_embedding:\n",
    "        if len(el)!=60:\n",
    "            print(el.shape)\n",
    "    return np.stack(batch_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word UNK to index       : 132037\n",
      "The intex for UNK to string : UNK\n",
      "The shape of enbedings      : (2196018, 300)\n",
      "The first 5words from vocab : ['/pad', ',', '.', 'the', 'and', 'to', 'of', 'a', 'in', '\"']\n"
     ]
    }
   ],
   "source": [
    "print('The word UNK to index       :', word_to_index['UNK'])\n",
    "print('The intex for UNK to string :', index_to_word[132037])\n",
    "print('The shape of enbedings      :', embeddings.shape)\n",
    "print('The first 5words from vocab :',vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regresion():\n",
    "    \n",
    "    def batch_matmul_right(X,w):\n",
    "        mult = []\n",
    "        for x in tf.unstack(X,axis=0):\n",
    "            mult.append(tf.matmul(x,w))\n",
    "        return tf.stack(mult)\n",
    "    \n",
    "    def batch_matmul_left(w,X):\n",
    "        mult = []\n",
    "        for x in tf.unstack(X,axis=0):\n",
    "            mult.append(tf.matmul(w,x))\n",
    "        return tf.stack(mult)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32,shape=(64, 60, 300), name = 'x')\n",
    "    y = tf.placeholder(tf.float32,shape=(64, ),name='y')\n",
    "    \n",
    "    with tf.variable_scope('lreg') as scope:\n",
    "        \n",
    "        w = tf.Variable(tf.random_uniform((300,1), -1, 1), name='W')\n",
    "        print(x.shape, w.shape)\n",
    "        W = tf.stack([w]*64)\n",
    "        #mul = tf.matmul(w,x)\n",
    "        mul = batch_matmul_right(x,w)\n",
    "        print(mul.shape)\n",
    "        w2 = tf.Variable(tf.random_uniform((1,60), -1, 1), name='W2')\n",
    "        mul2 = batch_matmul_left(w2,mul)\n",
    "        mul2 = tf.reshape(mul2,(64, ))\n",
    "        print(mul2.shape)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=mul2)        \n",
    "#         linear1 = tf.nn.relu(tf.matmul(x, w))\n",
    "\n",
    "#         w2 = tf.Variable(tf.random_uniform((1,None), -1, 1), name='W')\n",
    "        \n",
    "#         y_pred = tf.matmul(w2, linear1)\n",
    "        \n",
    "#         loss = tf.square(y_pred - y)\n",
    "        \n",
    "    return x, y, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(batches):\n",
    "    global questions_t\n",
    "    global y_train\n",
    "    global embeddigs\n",
    "    \n",
    "    \n",
    "    x, y, loss = linear_regresion()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        average_loss = 0\n",
    "        for batch in tqdm(batches):\n",
    "            batch, label =  batch\n",
    "            batch = embedding_lookup( batch )\n",
    "            label = np.array(label)\n",
    "            _ ,batch_loss = session.run([optimizer, loss], {x:batch, y :label})\n",
    "            average_loss += batch_loss\n",
    "        print(\"Avg loss :\",average_loss/len(batches))\n",
    "    \n",
    "    return y_pred_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = Batch(questions_t, y_train, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 60, 300) (300, 1)\n",
      "(64, 60, 1)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████▉| 20407/20409 [04:45<00:00, 69.57it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10, 60, 300) for Tensor 'x:0', which has shape '(64, 60, 300)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f166d8cb8e1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-726da1961b5e>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(batches)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0m_\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0maverage_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Avg loss :\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lpalasan\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lpalasan\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1111\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1112\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (10, 60, 300) for Tensor 'x:0', which has shape '(64, 60, 300)'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████████████████████████████████████████████▉| 20407/20409 [05:00<00:00, 69.57it/s]"
     ]
    }
   ],
   "source": [
    "run(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
