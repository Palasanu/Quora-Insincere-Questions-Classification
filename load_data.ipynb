{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "78a1697e883b5fa55847d808bb1b81643f919a46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# script made on 16gb of ram\n",
    "import os\n",
    "import io\n",
    "import os.path\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf      #the progress bar\n",
    "import en_core_web_sm as en  #from the spaCy library, https://spacy.io/usage/\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from  tensorflow.keras.backend import binary_crossentropy\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "32527401fa956da5514985117d3ba73f78a80b24"
   },
   "outputs": [],
   "source": [
    "filename = \"./data/train.csv\"\n",
    "testname = \"./data/test.csv\"\n",
    "output = \"./data/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d761b00e58dc29f20f27dd5e9d2faccbd386c678"
   },
   "outputs": [],
   "source": [
    "# enb_path = 'input/embeddings/wiki-news-300d-1M/' + 'wiki-news-300d-1M.vec'\n",
    "# enb_path = 'input/embeddings/GoogleNews-vectors-negative300/'\n",
    "# enb_path = 'input/embeddings/paragram_300_sl999/'\n",
    "enb_path = \"./data/embeddings/glove.840B.300d/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch():\n",
    "    def __init__(self, data, labels, size = 64):\n",
    "        self.data = data\n",
    "        self.lenght = len(self.data )\n",
    "        del data\n",
    "        self.labels = labels\n",
    "        self.size = size\n",
    "        self.index = 0\n",
    "    \n",
    "    def __next__(self):\n",
    "        \n",
    "        \n",
    "        if (self.index >= self.lenght):\n",
    "            raise StopIteration()  \n",
    "        \n",
    "        if (self.index + self.size > self.lenght):\n",
    "            self.size = self.lenght - self.index \n",
    "        \n",
    "        self.index += self.size  \n",
    "        \n",
    "        return self.data[self.index-self.size:self.index],self.labels[self.index-self.size:self.index]        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return ceil(len(self.data)/self.size)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9a828213eedcd8ddda4d47e78c9a72174235357"
   },
   "outputs": [],
   "source": [
    "#global variables to save memory\n",
    "# _t is for tokenized\n",
    "embeddings = []      #np array of embeddings\n",
    "vocab = set([])          #the vocabulary from the enbeding file\n",
    "questions_train = []    #every question tokenized in an array\n",
    "questions_test = []\n",
    "#text_t = []         #all the questions concatenated and tokenized\n",
    "word_to_index = {}\n",
    "index_to_word = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e465d9c60ed01f70c9322165f4509282c4f8957b"
   },
   "outputs": [],
   "source": [
    "def tokenize_questions(text,nr_to_delete = 0):\n",
    "    nlp = en.load()      #load the tokenizer\n",
    "    \n",
    "    #global text_t       #every token in the original order\n",
    "    \n",
    "    #text_t = [] \n",
    "    questions_tokenized = []\n",
    "    \n",
    "    bar  = tqdm(total = len(text))\n",
    "    \n",
    "    for batch in text:  #tqdm is the progress bar\n",
    "        tokens = nlp(batch)\n",
    "        batch_tokenized = []\n",
    "        for token in tokens:\n",
    "            word = token.string.strip()\n",
    "            #text_t.append(word)\n",
    "            batch_tokenized.append(word)\n",
    "        questions_tokenized.append(batch_tokenized)\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "    return questions_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30a4e6ffecc691f767afb05e430fe31a9689bc50",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(filename)    #test_t is for now a dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = test_df.values[:,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.values[:,2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7db80bc8f30e251d4a0459c55e2400781b8240c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# loadig the qustions tokenized, and all qustions concatenated tokenized\n",
    "\n",
    "if not os.path.isfile(output + \"questions_test\"):\n",
    "    questions_test = tokenize_questions( test_df.values[:,1].tolist() )   \n",
    "    with  open(output + \"questions_test\",'wb')  as file:\n",
    "        pickle.dump(questions_test, file)\n",
    "    if not os.path.isfile(output + \"questions_t\"):\n",
    "        questions_train = tokenize_questions( train_df.values[:,1].tolist() )\n",
    "        with  open(output + \"questions_t\",'wb')  as file:\n",
    "            pickle.dump(questions_train, file)\n",
    "            \n",
    "    #pickle.dump(text_t, open(output + \"text_t\",'wb') )\n",
    "else:\n",
    "    print(\"Loading data...\")\n",
    "    with open(output + \"questions_t\", \"rb\" ) as file:\n",
    "        questions_train = pickle.load( file )\n",
    "    with open(output + \"questions_test\", \"rb\" ) as file:\n",
    "        questions_test = pickle.load( file )\n",
    "        \n",
    "    #text_t = pickle.load( open(output + \"text_t\", \"rb\" ) )\n",
    "    print(\"Nr of questions train: \",len(questions_train))\n",
    "    print(\"Nr of questions test : \",len(questions_test))\n",
    "    #print(\"Nr of tokens:    \",len(text_t))\n",
    "\n",
    "print('The fist question tokenized from train: ', questions_train[0][:8])\n",
    "print('The fist question tokenized from test : ', questions_test[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the words from all the questions so we can remove \n",
    "# the ones that does not appear from embedings to save memory\n",
    "\n",
    "def get_questions_vocab(*args):     \n",
    "    vocab = set([])\n",
    "    for questions_set in args:\n",
    "        for question in questions_set:\n",
    "            for word in question:\n",
    "                vocab.add(word)\n",
    "    vocab.add('UNK') #we add an word for the unknown words in the future unseen questions \n",
    "    return vocab\n",
    "    \n",
    "vocab = get_questions_vocab(questions_train,questions_test)\n",
    "print(\"Nr of unique word in questions\",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42977bc4a0039f3ccb6c1614544faf2e22b01ece"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(enb_path,vocab):    \n",
    "    \n",
    "    print(\"Loading embeddings...\")\n",
    "    \n",
    "    with open(enb_path, 'r', encoding ='utf-8') as file:\n",
    "    \n",
    "        num_lines = sum(1 for line in file)\n",
    "        bar  = tqdm(total = num_lines)\n",
    "    \n",
    "        file.seek(0)\n",
    "    \n",
    "        vocab_enb = []\n",
    "        embeddings = []      \n",
    "        for line in file:\n",
    "            bar.update(1)\n",
    "            array = line[:-1].split(' ')\n",
    "            word = str(array[0])\n",
    "            if word in vocab:\n",
    "                vocab_enb.append(word)\n",
    "                embeddings.append(np.array(array[1:]).astype('float32'))      \n",
    "        bar.close()\n",
    "        \n",
    "    embeddings = np.stack(embeddings)\n",
    "    \n",
    "    return vocab_enb, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(output + \"embeddings\"):\n",
    "    vocab, embeddings = load_embeddings(enb_path,vocab)\n",
    "    with open( output + \"embeddings\",'wb') as file:\n",
    "        pickle.dump(embeddings, file)\n",
    "    with open( output + \"vocabulary\",'wb') as file:\n",
    "        pickle.dump(vocab,open( output + \"vocabulary\",'wb'))\n",
    "else:\n",
    "    print(\"Loading embeddings...\")   \n",
    "    with open(output + \"embeddings\", \"rb\" ) as file:\n",
    "        embeddings = pickle.load( file )\n",
    "    with open( output + \"vocabulary\", \"rb\" ) as file:\n",
    "        vocab = pickle.load( file )\n",
    "print('Embedings size:  ',embeddings.shape)\n",
    "print('Vocabulary size: ', len(vocab))\n",
    "\n",
    "print('\\nThe word \"'+ vocab[2]+'\" with his enmeding: \\n' , embeddings[2][:5], \"... \", embeddings[2][-5:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_padding(vocab,embeddings):\n",
    "    #we need to add the padding word to our vocabulary\n",
    "    #we need to add the padding word to our embedding matrix\n",
    "    \n",
    "    print(\"Adding the padding char to vocab and embeddings...\")\n",
    "    vocab = ['/pad']+vocab\n",
    "    pad_emb = []\n",
    "    for i in range(len(embeddings[0])):\n",
    "        pad_emb.append(0)\n",
    "    pad_emb = np.array(pad_emb)\n",
    "    embeddings = np.vstack([pad_emb,embeddings])\n",
    "    \n",
    "    return vocab, embeddings\n",
    "\n",
    "def create_dictionaries(vocab):\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    \n",
    "    for idx, word in enumerate(vocab): #create the two dictionaries\n",
    "        word_to_index[word] = idx\n",
    "        index_to_word[idx] = word\n",
    "        \n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "def index_question_to_words(question_indexes):\n",
    "    question_words = \"\"\n",
    "    for word in question_indexes:\n",
    "        question_words += index_to_word[word] + ' '\n",
    "    return question_words\n",
    "\n",
    "def token_to_index(questions_tokenized, vocab):\n",
    "    \n",
    "    for question in questions_tokenized: #chage the input from strings to ids\n",
    "        for idx,word_token in enumerate(question): #replace the text with the word indexes\n",
    "             question[idx] = word_to_index.get(word_token, 87152) #if the token is not in vocab put \"UNK\"\n",
    "    return questions_tokenized\n",
    "\n",
    "def pad_questions(data, max_len = 60):\n",
    "    \n",
    "    print(\"Padding questions...\")\n",
    "    data_aux = deepcopy(data)\n",
    "    \n",
    "    padded_count = 0\n",
    "    cut_count = 0\n",
    "    for idx in tqdm(range(len(data_aux))):\n",
    "        question_len = len(data_aux[idx])\n",
    "        if question_len <= max_len:\n",
    "            padded_count += 1\n",
    "            for i in range(question_len, max_len):\n",
    "                data_aux[idx].append(0)\n",
    "        elif question_len > max_len:\n",
    "            cut_count += 1\n",
    "            data_aux[idx] = data_aux[idx][:max_len]\n",
    "        \n",
    "        if len(data_aux[idx])>max_len:\n",
    "            print(question_len)\n",
    "            print(question)\n",
    "            break\n",
    "            \n",
    "    print(\"Padded :\", padded_count)\n",
    "    print(\"Cut    :\", cut_count)\n",
    "    return data_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,embeddings = initialze_padding(vocab,embeddings)\n",
    "\n",
    "word_to_index, index_to_word = create_dictionaries(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The word UNK to index       :', word_to_index['UNK'])\n",
    "print('The intex for UNK to string :', index_to_word[87152])\n",
    "print('The shape of enbedings      :', embeddings.shape)\n",
    "print('The first 5words from vocab :',vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train = token_to_index(questions_train,vocab)\n",
    "questions_test = token_to_index(questions_test,vocab)\n",
    "\n",
    "questions_train = pad_questions(questions_train)\n",
    "questions_test = pad_questions(questions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First train question to intdexes :\")\n",
    "print(index_question_to_words(questions_train[0][:17]))\n",
    "print(questions_train[0][:18])\n",
    "\n",
    "print(\"\\nFirst test question to intdexes  :\")\n",
    "print(index_question_to_words(questions_test[0][:17])+\"...\")\n",
    "print(questions_test[0][:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(batch):\n",
    "    global embeddings\n",
    "    \n",
    "    batch_embedding = []\n",
    "    \n",
    "    for int_question in batch:\n",
    "        question_embedding = []\n",
    "\n",
    "        for int_word in int_question:\n",
    "            try:\n",
    "                question_embedding.append(embeddings[int_word])\n",
    "            except Exception:\n",
    "                print(int_word)\n",
    "        \n",
    "        enb = np.stack(question_embedding)\n",
    "        batch_embedding.append(enb)\n",
    "    for el in batch_embedding:\n",
    "        if len(el)!=60:\n",
    "            print(el.shape)\n",
    "    return np.stack(batch_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(label, prediction):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    " \n",
    "    for i in range(0, len(label)):\n",
    "        if prediction[i] == 1:\n",
    "            if prediction[i] == label[i]:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        else:\n",
    "            if prediction[i] == label[i]:\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                false_negatives += 1\n",
    "    return  true_positives, false_positives, true_negatives, false_negatives\n",
    "\n",
    "def scores(thresholds,output,label):\n",
    "    best_f1 = 0.0\n",
    "    best_out = []\n",
    "    thresh = 0.0\n",
    "    for th_val in thresholds:\n",
    "                aux_output = np.copy(output)\n",
    "                aux_output[aux_output<=th_val] = bool(0)\n",
    "                aux_output[aux_output>th_val] = bool(1)\n",
    "\n",
    "                true_positives, false_positives, true_negatives, false_negatives = confusion_matrix(label,aux_output)\n",
    "                accuracy = (true_positives + true_negatives) / (true_positives \\\n",
    "                            + true_negatives + false_positives + false_negatives)\n",
    "                \n",
    "                if true_positives + false_positives == 0:\n",
    "                    precision = 0.0001\n",
    "                else:\n",
    "                    precision  = true_positives / (true_positives + false_positives)\n",
    "                \n",
    "                if true_positives == 0:\n",
    "                    recall = 0.0001\n",
    "                else:\n",
    "                    recall = true_positives / (true_positives + false_negatives)\n",
    "                \n",
    "                f1_score = 2 / ((1 / precision) + (1 / recall))\n",
    "\n",
    "                print('   Thresholds {:.2f}:: Acc: {:.4f} | Precision: {:.4f} | Recall: {:.4f} | F1: {:.4f}'.format(\n",
    "                th_val, accuracy, precision, recall, f1_score))\n",
    "\n",
    "                if f1_score > best_f1:\n",
    "                    best_out = aux_output\n",
    "                    best_f1 = f1_score\n",
    "                    thresh = th_val\n",
    "    return thresh,best_f1,best_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_class(x,y, percentage = 80, random = True):\n",
    "    \n",
    "    data_in_classes = [[],[]]\n",
    "    \n",
    "    left_split = []\n",
    "    right_split = []\n",
    "    \n",
    "    for idx,val in enumerate(y):                    #separate the data with target 0 from 1\n",
    "        data_in_classes[int(val)].append([idx,val]) #save them as (idx,val) to save memory\n",
    "    \n",
    "    \n",
    "    for class_data in data_in_classes:          #split the classes in two and add them to the left and right split \n",
    "        total = len(class_data)\n",
    "        split = int(total*(percentage/100))\n",
    "        left_split += class_data[:split]\n",
    "        right_split += class_data[split:]\n",
    "    \n",
    "    if random:                                  #shuffle them so we lose the order of the quetions\n",
    "        np.random.shuffle(right_split)\n",
    "        np.random.shuffle(left_split)\n",
    "      \n",
    "    # make the x_train,y_train from the left_split\n",
    "    left_split = np.array(left_split)\n",
    "    y_left = left_split[:,1]\n",
    "    left_split = left_split[:,0]\n",
    "    x_left = []\n",
    "    for idx_x in left_split:\n",
    "        x_left.append(x[idx_x])\n",
    "    del left_split\n",
    "    \n",
    "    # make the x_test,y_test from the right_split\n",
    "    right_split = np.array(right_split)\n",
    "    y_right = right_split[:,1]\n",
    "    right_split = right_split[:,0]\n",
    "    x_right = []\n",
    "    for idx_x in right_split:\n",
    "        x_right.append(x[idx_x])\n",
    "    del right_split\n",
    "    \n",
    "            \n",
    "    return x_left,x_right,y_left,y_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test  = split_on_class(questions_train,y)\n",
    "x_train, x_val, y_train, y_val  = split_on_class(questions_train, y)\n",
    "print(\"The leght of train data      :\",len(x_train))\n",
    "print(\"The leght of validation data :\",len(x_val))\n",
    "#print(\"The leght of test data       :\",len(x_test))\n",
    "print(\"There are\",sum(y_train),\"labeled positive in the train data\")\n",
    "print(\"There are\",sum(y_val),\"labeled positive in the validation data\")\n",
    "#print(\"There are\",sum(y_test),\"labeled positive in the test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(x): \n",
    "    with tf.variable_scope('lreg') as scope:\n",
    "        aux = GlobalMaxPool1D()(x)\n",
    "        aux = Dense(16, activation=\"relu\")(aux)\n",
    "        aux = Dropout(0.1)(aux)\n",
    "        aux = Dense(1, activation=\"sigmoid\")(aux)\n",
    "        out = tf.reshape(aux, [-1])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = {'output':[],'f1_score':0,'threshold':0, 'epoch':0}\n",
    "\n",
    "q_output = []\n",
    "\n",
    "def run():\n",
    "    global questions_test\n",
    "    global embeddigs\n",
    "    global x_train\n",
    "    global y_train\n",
    "    global x_test\n",
    "    global y_test\n",
    "    global x_val\n",
    "    global y_val\n",
    "    global best_result\n",
    "    global bests_epochs\n",
    "    global q_output\n",
    "\n",
    "\n",
    "    batch_size = 900\n",
    "    thresholds = np.arange(0.2, 0.5, 0.02)\n",
    "    epochs = 15\n",
    "\n",
    "    x = tf.placeholder(tf.float32,shape=(None, 60, 300), name = 'x')\n",
    "    y = tf.placeholder(tf.float32,shape=(None, ),name='y')\n",
    "    predictions = weights(x)\n",
    "    loss = binary_crossentropy(target = y, output = predictions)\n",
    "\n",
    "\n",
    "    x_eval = tf.placeholder(tf.float32,shape=(None, 60, 300), name = 'x_eval')\n",
    "    y_eval = tf.placeholder(tf.float32,shape=(None, ),name='y_eval')\n",
    "    predictions_eval = weights(x_eval)\n",
    "    loss_eval = binary_crossentropy(target = y_eval, output = predictions_eval)\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        session.run(init_op)\n",
    "\n",
    "        for e in range(1, epochs + 1):\n",
    "            print(\"\\n\" + \"_\" * 80)\n",
    "            print(\"Epoch nr\", e, \":\")\n",
    "            for phase in ['train', 'val']:\n",
    "                average_loss = 0\n",
    "                output = np.array([])\n",
    "                labels = np.array([])\n",
    "\n",
    "                if phase == 'train':\n",
    "                    batches = Batch(x_train, y_train, batch_size)\n",
    "                elif phase == 'test':\n",
    "                    batches = Batch(x_test, y_test, batch_size)\n",
    "                elif phase == 'val':\n",
    "                    batches = Batch(x_val, y_val, batch_size)\n",
    "\n",
    "                for batch in tqdm(batches):\n",
    "                    batch, label =  batch\n",
    "                    batch = embedding_lookup( batch )\n",
    "                    label = np.array(label)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        _ ,batch_output ,batch_loss = session.run([optimizer, predictions, loss], {x:batch, y :label})\n",
    "                    else:\n",
    "                        batch_output ,batch_loss = session.run([predictions, loss], {x:batch, y :label})\n",
    "\n",
    "                    labels = np.concatenate((labels, label), axis=0)\n",
    "                    output = np.concatenate((output, batch_output), axis=0)\n",
    "                    average_loss += np.average(batch_loss)\n",
    "                print(\"The \"+phase+\" stats :\")\n",
    "                print('   Loss: {:.4f}'.format(average_loss/len(batches)))\n",
    "                th,best_epoch_f1, best_epoch_output = scores(thresholds,output,labels)\n",
    "                if phase == 'val':\n",
    "                    best_epoch_score = {'output':best_epoch_output,'f1_score':best_epoch_f1,'threshold':th} \n",
    "                    if best_epoch_f1 > best_result['f1_score']:\n",
    "                        best_result['output'] = best_epoch_output\n",
    "                        best_result['f1_score'] = best_epoch_f1\n",
    "                        best_result['threshold'] = th\n",
    "                        best_result['epoch'] = e\n",
    "                        q_output = []\n",
    "                        #get predictions for the test datafrom the competision\n",
    "                        batches = Batch(questions_test, questions_test, batch_size)\n",
    "                        for batch in tqdm(batches):\n",
    "                            batch, label =  batch\n",
    "                            batch = embedding_lookup( batch )\n",
    "                            label = np.array(label)\n",
    "                            batch_output = session.run(predictions, {x: batch})\n",
    "                            q_output = np.concatenate((q_output, batch_output), axis=0)\n",
    "        return q_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Thresholds {}:: F1: {:.4f}, epoch: {}'.format(best_result['threshold'], best_result['f1_score'],best_result['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
