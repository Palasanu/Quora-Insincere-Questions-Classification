{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78a1697e883b5fa55847d808bb1b81643f919a46"
      },
      "cell_type": "code",
      "source": "import os\nimport os.path\nimport random\nimport pickle\nimport zipfile\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nimport tensorflow as tf      #the progress bar\nimport en_core_web_sm as en  #from the spaCy library, https://spacy.io/usage/",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32527401fa956da5514985117d3ba73f78a80b24"
      },
      "cell_type": "code",
      "source": "filename = \"../input/quora-insincere-questions-classification/train.csv\"",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d761b00e58dc29f20f27dd5e9d2faccbd386c678"
      },
      "cell_type": "code",
      "source": "# enb_path = 'input/embeddings/wiki-news-300d-1M/' + 'wiki-news-300d-1M.vec'\n# enb_path = 'input/embeddings/GoogleNews-vectors-negative300/'\n# enb_path = 'input/embeddings/paragram_300_sl999/'\nenb_path = \"../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/\" + \"glove.840B.300d.txt\"",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9a828213eedcd8ddda4d47e78c9a72174235357"
      },
      "cell_type": "code",
      "source": "#global variables to save memory\n# _t is for tokenized\nembedings = []      #np array of enbedings\nvocab = []          #the vocabulary from the enbeding file\nquestions_t = []    #every question tokenized in an array\ntext_t = []         #all the questions concatenated and tokenized\nword_to_index = {}\nindex_to_word = {}",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e465d9c60ed01f70c9322165f4509282c4f8957b"
      },
      "cell_type": "code",
      "source": "def tokenize_questions(text,nr_to_delete = 0):\n    nlp = en.load()      #load the tokenizer\n    \n    global questions_t\n    global vocab               #every unique word in the appearance order  \n    global text_t      #every token in the original order\n    \n    text_t = [] \n    vocab = [] \n    questions_t = []\n    \n    batches = text.tolist()\n    \n    del text  #delete the original text so we save memory\n    bar  = tqdm(total = len(batches))\n    \n    for batch in batches:  #tqdm is the progress bar\n        tokens = nlp(batch)\n        batch_tokenized = []\n        for token in tokens:\n            word = token.string.strip()\n            text_t.append(word)\n            batch_tokenized.append(word)\n        questions_t.append(batch_tokenized)\n        bar.update(1)\n    bar.close()",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30a4e6ffecc691f767afb05e430fe31a9689bc50"
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(filename)    #test_t is for now a dataFrame",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7db80bc8f30e251d4a0459c55e2400781b8240c8"
      },
      "cell_type": "code",
      "source": "if not os.path.isfile(\"../output/questions_t\"):\n    tokenize_questions( train_df.values[:,1] )\n    pickle.dump(questions_t, open(\"../output/batches_tokenized\",'wb') )\n    pickle.dump(text_t, open(\"../output/text_tokenized\",'wb') )\nelse:\n    print(\"Loading data...\")\n    questions_t = pickle.load( open( \"../output/questions_t\", \"rb\" ) )\n    text_t = pickle.load( open( \"../output/text_t\", \"rb\" ) )\n    print(\"Nr of questions: \",len(questions_t))\n    print(\"Nr of tokens:    \",len(text_t))",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading data...\nNr of questions:  1306122\nNr of tokens:     19020768\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42977bc4a0039f3ccb6c1614544faf2e22b01ece"
      },
      "cell_type": "code",
      "source": "def load_embedings(enb_path):\n    global embedings\n    global vocab\n    \n    embedings = []\n    vocab = []\n    \n    print(\"Loading enbedings...\")\n    \n    num_lines = sum(1 for line in open(enb_path))\n    bar  = tqdm(total = num_lines)\n    \n    with open(enb_path, \"r\") as ins:\n        array = ins.readline()[:-1].split(' ')\n        vocab = [str(array[0])]\n        embedings = [np.array(array[1:]).astype('float32')]\n        \n        for line in ins:\n            bar.update(1)\n            array = line[:-1].split(' ')\n            vocab.append(str(array[0]))\n            embedings.append(np.array(array[1:]).astype('float32'))      \n        bar.close()\n        embedings = np.stack(embedings)\n    print(\"Done!\")",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "299a567138ac07d421513f3d7223090025008968"
      },
      "cell_type": "code",
      "source": "if not os.path.isfile(\"../output/embedings\"):\n    load_embedings(enb_path)\n    pickle.dump(embedings,open( \"../output/embedings\",'wb'))\n    pickle.dump(vocab,open( \"../output/vocabulary\",'wb'))\nelse:\n    print(\"Loading enbedings...\")\n    embedings = pickle.load( open( \"../output/embedings\", \"rb\" ) )\n    vocab = pickle.load( open( \"../output/vocabulary\", \"rb\" ) )\nprint('Embedings size:  ',embedings.shape)\nprint('Vocabulary size: ', len(vocab))\n",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading enbedings...\nEmbedings size:   (2196017, 300)\nVocabulary size:  2196017\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c36543928269e29af27f41e16d6c848c829e6901"
      },
      "cell_type": "code",
      "source": "def tokens_to_index():\n    global word_to_index\n    global index_to_word\n    global questions_t\n    global vocab\n    \n    word_to_index = {}\n    index_to_word = {}\n    \n    for idx, word in enumerate(vocab): #create the two dictionaries\n        word_to_index[word] = idx\n        index_to_word[idx] = word\n        \n    for batch in questions_t: #chage the input from strings to ids\n        if isinstance(batch, int):\n            print(batch)\n        for idx,token in enumerate(batch): #replace the text with the word indexes\n             batch[idx] = word_to_index.get(token, \"UNK\")\ntokens_to_index()",
      "execution_count": 55,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f3438ff51e061d083ba52d4c58377190776a45f"
      },
      "cell_type": "code",
      "source": "questions_t[0]",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 70,
          "data": {
            "text/plain": "[255, 127, 9833, 62681, 118, 58, 7599, 28, 6, 1990, 7, 2, 10310, 37]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4be70e52be71a930b02b9048bf3a9bfd066f58b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}